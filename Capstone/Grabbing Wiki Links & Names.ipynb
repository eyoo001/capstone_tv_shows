{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import wikipedia\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@Alexander_H/scraping-wikipedia-with-python-8000fc9c9e6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = pd.read_csv('./Untitled spreadsheet - Sheet1.csv')\n",
    "net_list = list(networks.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_American_Broadcasting_Company',\n",
       " 'https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_Adult_Swim',\n",
       " 'https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_A%26E',\n",
       " 'https://en.wikipedia.org/wiki/List_of_original_programs_distributed_by_Amazon',\n",
       " 'https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_AMC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get('https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_American_Broadcasting_Company')\n",
    "b = BeautifulSoup(html.text, 'lxml')\n",
    "links = []\n",
    "\n",
    "for i in b.find_all(name = 'li'):\n",
    "    for link in i.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "links = links[77:80]\n",
    "abc_links = ['https://en.wikipedia.org' + i for i in links]\n",
    "\n",
    "len(abc_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shows from https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_American_Broadcasting_Company\n",
      "Collecting shows from https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_Adult_Swim\n",
      "Collecting shows from https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_A%26E\n",
      "Collecting shows from https://en.wikipedia.org/wiki/List_of_original_programs_distributed_by_Amazon\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5ac8e9a2f8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;31m# get the title and add to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0mtv_titles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0;31m# get the link and add to that list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mtv_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n\u001b[1;32m   1010\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "tv_titles = []\n",
    "tv_links = []\n",
    "\n",
    "\n",
    "for genre in net_list:\n",
    "    print(f'Collecting shows from {genre}')\n",
    "    html = requests.get(genre)\n",
    "    b = BeautifulSoup(html.text, 'lxml')\n",
    "    # get to the table on the page\n",
    "    for i in b.find_all(name='table', class_='wikitable'):\n",
    "        # get to the row of each show\n",
    "        for j in i.find_all(name='tr'):\n",
    "            #get just the title cell for each row.\n",
    "            # contains the title and the URL\n",
    "            for k in j.find_all(name='i'):\n",
    "                # get within that cell to just get the words\n",
    "                for link in k.find_all('a', href=True):\n",
    "                    # get the title and add to the list\n",
    "                    tv_titles.append(link['title'])\n",
    "                    # get the link and add to that list\n",
    "                    tv_links.append(link['href'])\n",
    "    #be a conscientious scraper and pause between scrapes\n",
    "    time.sleep(1)\n",
    "print(f'Number of TV Links Collected: {len(tv_links)}')\n",
    "print(f'Number of TV Titles Collected: {len(tv_titles)}')\n",
    "# remove film links that don't have a description page on Wikipedia\n",
    "new_tv_links = [i for i in tv_links if 'redlink' not in i]\n",
    "# same goes for titles\n",
    "new_tv_titles = [i for i in tv_titles if '(page does not exist)' not in i]\n",
    "print(f'Number of TV Links with Wikipedia Pages: {len(new_tv_links)}')\n",
    "print(f'Number of TV Titles with Wikipedia Pages: {len(new_tv_titles)}')\n",
    "#use this list to fetch from the API\n",
    "title_links = list(zip(new_tv_titles, new_tv_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Simpsons', 'American Dad!', 'Criminal Minds']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tv_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get('https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_Fox')\n",
    "b = BeautifulSoup(html.text, 'lxml')\n",
    "links = []\n",
    "\n",
    "for i in b.find_all(name = 'li'):\n",
    "    for link in i.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "links = links[57:]\n",
    "fox_links = ['https://en.wikipedia.org' + i for i in links]\n",
    "\n",
    "len(fox_links)\n",
    "# Need to remove cites, external links, related links, List links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get('https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_NBC')\n",
    "b = BeautifulSoup(html.text, 'lxml')\n",
    "links = []\n",
    "\n",
    "for i in b.find_all(name = 'li'):\n",
    "    for link in i.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "links = links[7:]\n",
    "nbc_links = ['https://en.wikipedia.org' + i for i in links]\n",
    "\n",
    "len(nbc_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get('https://en.wikipedia.org/wiki/List_of_programs_broadcast_by_CBS')\n",
    "b = BeautifulSoup(html.text, 'lxml')\n",
    "links = []\n",
    "\n",
    "for i in b.find_all(name = 'li'):\n",
    "    for link in i.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "links = links[7:]\n",
    "cbs_links = ['https://en.wikipedia.org' + i for i in links]\n",
    "\n",
    "len(cbs_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently have a list of wikis\n",
    "- Need to extract the show name (can be done simultaneously via the href title?)\n",
    "- Need to then take that list of names and use the wiki API to query the tables?? \n",
    "- NEED to grab: viewership information, logline\n",
    "- WANT to grab: writer, air date, rating, director, ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVDV or Trakt API\n",
    "\n",
    "- Take the list of names from the wiki to return IDs\n",
    "- Use the IDs to grab the metadata\n",
    "- Combine the metadata with the wiki information in a dataframe\n",
    "- NEED: number of episodes, number of seasons, air date, producer, network, ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=jsonfm&formatversion=2&titles=List_of_Modern_Family_episodes\n",
    "\n",
    "https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=jsonfm&formatversion=2&titles=Grey%27s_Anatomy_(season_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two formats - List_of_SHOW_NAME_episodes & SHOW_NAME_(season_#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample API for wikipedia from https://www.mediawiki.org/wiki/API:Query#Sample_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
