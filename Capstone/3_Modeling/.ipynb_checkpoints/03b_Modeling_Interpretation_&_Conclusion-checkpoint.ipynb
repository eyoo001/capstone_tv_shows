{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Model\n",
    "\n",
    "Now that I have my features engineered and extraneous information removed, I can prepare grid searches to tune for the best model to predict my target variable. I will be using Linear Regression, Lasso/Ridge/Elastic Net, Random Forest, and Gradient Boost to see which of these will work best. I have concerns about the ensemble methods being effective since I have such a small dataset, so I will be looking for which models are not overfitting as much. \n",
    "\n",
    "As a disclaimer, I did some iterative testing of various engineered features that I've since lost in my notebooks, so if it seems like a particular feature is missing at this point, it is likely that the feature was removed during that testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../0_Assets_&_Data/final_model.pickle', 'rb') as handle:\n",
    "    good_model = pickle.load(handle)\n",
    "\n",
    "with open('../0_Assets_&_Data/model_prelim.pickle', 'rb') as handle:\n",
    "    model_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have a better idea of how I'd like to approach the features. The categories aren't necessarily always helping, so I wlil leave certain values (i.e. month, day of week, etc.) as dummied text while still manipulating weights given to certain other categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['number_of_episodes_x', 'number_of_seasons_x', 'awards_x',\n",
       "       'imdb_rating_x', 'imdb_votes_x', 'timeslot_00:00', 'timeslot_afternoon',\n",
       "       'timeslot_evening', 'timeslot_latenight', 'timeslot_morning',\n",
       "       'timeslot_unknown', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4',\n",
       "       'day_5', 'day_6', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',\n",
       "       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n",
       "       'month_12', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy',\n",
       "       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show',\n",
       "       'History', 'Horror', 'Music', 'Mystery', 'News', 'Reality-TV',\n",
       "       'Romance', 'Sci-Fi', 'Sport', 'Talk-Show', 'Western', 'status_Canceled',\n",
       "       'status_Ended', 'status_In Production', 'status_Returning Series',\n",
       "       'type_Documentary', 'type_Miniseries', 'type_News', 'type_Reality',\n",
       "       'type_Scripted', 'type_Talk Show', 'type_Video', 'runtime_full',\n",
       "       'runtime_half', 'runtime_special', 'actor_1', 'actor_2', 'actor_3',\n",
       "       'actor_4', 'actor_1_weighted', 'actor_2_weighted', 'actor_3_weighted',\n",
       "       'actor_4_weighted', 'actors_cum_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model.drop(['number_of_episodes_y', 'number_of_seasons_y', \n",
    "                 'awards_y', 'imdb_votes_y', 'imdb_rating_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm only leaving in the below line for my own future evaluatino because it was providing me with a higher score (with less overfitting) for gradient boost only - however, I believe that the additional information from the engineered features are better for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = good_model[['imdb_votes_x', 'awards_x', 'number_of_seasons_x', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4',\n",
    "       'day_5', 'day_6', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy',\n",
    "       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show',\n",
    "       'History', 'Horror', 'Music', 'Mystery', 'News', 'Reality-TV',\n",
    "       'Romance', 'Sci-Fi', 'Sport', 'Talk-Show', 'Western', 'actor_1_weighted', 'actor_2_weighted', 'actor_3_weighted',\n",
    "       'actor_4_weighted', 'number_of_episodes_x']]\n",
    "y = good_model['imdb_rating_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = good_model.drop(['actor_1',\n",
    "                     'actor_2', \n",
    "                     'actor_3', \n",
    "                     'actor_4', \n",
    "                     'actors_cum_sum',\n",
    "                     'imdb_rating_x'], axis=1)\n",
    "y = good_model['imdb_rating_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['number_of_episodes_x', 'number_of_seasons_x', 'awards_x',\n",
       "       'imdb_votes_x', 'timeslot_00:00', 'timeslot_afternoon',\n",
       "       'timeslot_evening', 'timeslot_latenight', 'timeslot_morning',\n",
       "       'timeslot_unknown', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4',\n",
       "       'day_5', 'day_6', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',\n",
       "       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n",
       "       'month_12', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy',\n",
       "       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show',\n",
       "       'History', 'Horror', 'Music', 'Mystery', 'News', 'Reality-TV',\n",
       "       'Romance', 'Sci-Fi', 'Sport', 'Talk-Show', 'Western', 'status_Canceled',\n",
       "       'status_Ended', 'status_In Production', 'status_Returning Series',\n",
       "       'type_Documentary', 'type_Miniseries', 'type_News', 'type_Reality',\n",
       "       'type_Scripted', 'type_Talk Show', 'type_Video', 'runtime_full',\n",
       "       'runtime_half', 'runtime_special', 'actor_1_weighted',\n",
       "       'actor_2_weighted', 'actor_3_weighted', 'actor_4_weighted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Grid Search(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have an idea of which features do and don't work, I can focus on the hyperparameters as well to achieve a better score. I will be evaluating based on R2 and MAE. \n",
    "\n",
    "\n",
    "## $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs = GridSearchCV(LinearRegression(), \n",
    "                         {'n_jobs': [1, 5, 10]}, \n",
    "                         n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.295886673109228, 0.22276426541785377)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.fit(X_train_sc, y_train)\n",
    "\n",
    "lr_gs.score(X_train_sc, y_train), lr_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a relatively simple model, so there is not much tuning I can do with this. The score is a bit better than the initial model, but still not very good at just 24% variance being explained by my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs = GridSearchCV(LassoCV(),\n",
    "                        {'n_alphas': [100, 250],\n",
    "                        'tol': [.0001, .001, .01]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28649568241264667, 0.24303821735957343)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_gs.fit(X_train_sc, y_train)\n",
    "lasso_gs.score(X_train_sc, y_train), lasso_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso model performs pretty well in terms of not overfitting as much, and scoring higher than the regular linear regression. This makes sense, as the model may have too many features that this model can zero out. Although better, 27% is still not a very reliable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_gs = GridSearchCV(RidgeCV(),\n",
    "                        {'alphas': [(0.1, 1.0, 10.0), (0.01, 1.0, 10.0)]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2958816216455423, 0.22356661213373363)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_gs.fit(X_train_sc, y_train)\n",
    "ridge_gs.score(X_train_sc, y_train), ridge_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is performing worse than Lasso, which again makes sense given that Ridge still tries to see all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_gs = GridSearchCV(ElasticNetCV(),\n",
    "                        {'l1_ratio': [0.25, 0.5, 0.75],\n",
    "                         'n_alphas': [100, 250],\n",
    "                         'tol': [0.0001, 0.001, 0.01]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28779273589773857, 0.2452745843491575)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_gs.fit(X_train_sc, y_train)\n",
    "en_gs.score(X_train_sc, y_train), en_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1_ratio': 0.25, 'n_alphas': 100, 'tol': 0.0001}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs similar to lasso (slightly worse in this seeded random state), which makes sense given that it is utilizing lasso as part of the ratio in the parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs = GridSearchCV(RandomForestRegressor(), \n",
    "                         {'n_estimators': [10, 100, 200],\n",
    "                         'max_depth': [5, 25, 50],\n",
    "                         'min_samples_split': [2, 5, 10],\n",
    "                         'n_jobs': [1, 5, 10]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.889543527947757, 0.3838143863732124)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.fit(X_train_sc, y_train)\n",
    "rf_gs.score(X_train_sc, y_train), rf_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=5,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=10,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 25, 'min_samples_split': 5, 'n_estimators': 100, 'n_jobs': 10}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is surprisingly overfit. Looking at the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_gs = GridSearchCV(GradientBoostingRegressor(),\n",
    "                        {'learning_rate': [0.01, 0.1, 0.15],\n",
    "                        'n_estimators': [100, 150],\n",
    "                        'min_samples_split': [2, 4, 6],\n",
    "                        'max_depth': [3, 5, 10],\n",
    "                        'max_features': [0.5, 0.75, None],\n",
    "                        'alpha': [0.9, 0.95]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9894212324873556, 0.3268392503975629)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_gs.fit(X_train_sc, y_train)\n",
    "gb_gs.score(X_train_sc, y_train), gb_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=10, max_features=0.5,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.9,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 10,\n",
       " 'max_features': 0.5,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boost model is also overfitting, but the train and test scores are very good relative to the other models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sidenote, I was actually seeing better results on most of the models by removing certain features (namely the months and timeslots), but removing these did not affect the overfitting that was happening, and the only model being reduced was Gradient Boost. Because the difference in score was relatively marginal (and therefore may be attributable to the particular split/seed), I decided to leave the features in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE\n",
    "\n",
    "In addition to using the $R^2$ function as the scoring metric, I also want to take a look at the Mean Absolute Error (MAE), which can show me how far from the actual values my prediction is actually giving. The purpose of using MAE in this instance would be to see how far off my predictions are in terms of scaled numbers, rather than using $R^2$ to give a percentage of how much the variance is explained by the regression line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1195643334570298"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(model_df['imdb_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, one standard deviation for the IMDB ratings distribution is 1.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs = GridSearchCV(LinearRegression(), \n",
    "                         {'n_jobs': [1, 5, 10]}, \n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.704039545063736, -0.7493751736395751)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.fit(X_train_sc, y_train)\n",
    "\n",
    "lr_gs.score(X_train_sc, y_train), lr_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs = GridSearchCV(LassoCV(),\n",
    "                        {'n_alphas': [100, 250],\n",
    "                        'tol': [.0001, .001, .01]},\n",
    "                        scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7033709928257695, -0.7317579521110553)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_gs.fit(X_train_sc, y_train)\n",
    "lasso_gs.score(X_train_sc, y_train), lasso_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_gs = GridSearchCV(RidgeCV(),\n",
    "                        {'alphas': [(0.1, 1.0, 10.0), (0.01, 1.0, 10.0)]},\n",
    "                        scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6982951816599878, -0.747274532884661)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_gs.fit(X_train_sc, y_train)\n",
    "ridge_gs.score(X_train_sc, y_train), ridge_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_gs = GridSearchCV(ElasticNetCV(),\n",
    "                        {'l1_ratio': [0.25, 0.5, 0.75],\n",
    "                         'n_alphas': [100, 250],\n",
    "                         'tol': [0.0001, 0.001, 0.01]},\n",
    "                     scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7036802711614397, -0.7315634191542904)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_gs.fit(X_train_sc, y_train)\n",
    "en_gs.score(X_train_sc, y_train), en_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs = GridSearchCV(RandomForestRegressor(), \n",
    "                         {'n_estimators': [10, 100, 200],\n",
    "                         'max_depth': [5, 25, 50],\n",
    "                         'min_samples_split': [2, 5, 10],\n",
    "                         'n_jobs': [1, 5, 10]},\n",
    "                     scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.23707970588235305, -0.5847566137566136)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.fit(X_train_sc, y_train)\n",
    "rf_gs.score(X_train_sc, y_train), rf_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_gs = GridSearchCV(GradientBoostingRegressor(),\n",
    "                        {'learning_rate': [0.01, 0.1],\n",
    "                        'n_estimators': [100, 250],\n",
    "                        'min_samples_split': [2, 4, 6],\n",
    "                        'max_depth': [3, 5, 10],\n",
    "                        'alpha': [0.9, 0.95]},\n",
    "                     scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5421906108746971, -0.5957188381947357)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_gs.fit(X_train_sc, y_train)\n",
    "gb_gs.score(X_train_sc, y_train), gb_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.048544203282162"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(model_df['imdb_rating'])\n",
    "# mean of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1195643334570298"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(model_df['imdb_rating'])\n",
    "# standard deviation of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The models are all overfitting quite some bit, but gradient boost appears to be performing the best with about 40% explained variance. Random Forest is not far off either, but it is grossly overfitted and so I would not want to consider it a reliable model. Linear regression (and ridge and lasso) scored a bit more consistently, but also had lower scores. \n",
    "\n",
    "The outcome is not very surprising, especially given the high amount of features (relatively speaking) versus the low number of observations. Boosting will generally be less likely to ovefit due to the ensemble approach the model has in \"averaging\" the trees. This makes it better for weak learning datasets (i.e. high bias and low variance, as my dataset ought to have). On the other hand, Random Forest will be better suited towards low bias, high variance models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
