{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering Features\n",
    "\n",
    "Now that I know how badly my model is scoring, I am going to try and utilize some additional features in order to hopefully feed in better data for my model to train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from rake_nltk import Rake\n",
    "import datetime, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Assets_&_Data/series_df_full.pickle', 'rb') as f:\n",
    "    series_df = pickle.load(f)\n",
    "    \n",
    "with open('./Assets_&_Data/model_prelim.pickle', 'rb') as f:\n",
    "    model_df = pickle.load(f)\n",
    "    \n",
    "with open('./Assets_&_Data/week_day.pickle', 'rb') as f:\n",
    "    week_day = pickle.load(f)\n",
    "    \n",
    "with open('./Assets_&_Data/cleaned_series_df.pickle', 'rb') as f:\n",
    "    clean_series_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing Seasons\n",
    "\n",
    "After loading in the dataframe from my previous notebook, the first thing I want to try and do is to reduce the number of dummied months by sorting them into seasons instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_day['spring'] = (week_day['month'] < 7) & (week_day['month'] > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day['summer'] = (week_day['month'] < 9) & (week_day['month'] > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day['fall'] = (week_day['month'] < 12) & (week_day['month'] > 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day['winter'] = (week_day['month'] < 3) | (week_day['month'] == 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day_ = week_day.drop('released', axis=1)\n",
    "week_day_dum1 = pd.get_dummies(week_day_['weekday'], prefix='day')\n",
    "week_day_dum2 = pd.get_dummies(week_day_['month'], prefix='month')\n",
    "week_day_dum3 = week_day_[['spring', 'summer', 'fall', 'winter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_series_df['network'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dummies = pd.get_dummies(clean_series_df['network'], prefix='network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for row in range(0, len(clean_series_df['network'])):\n",
    "    if clean_series_df['network'][row] == '':\n",
    "        clean_series_df['network'][row] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an Actor Dataframe\n",
    "\n",
    "I want to try and imbue some weight to the actors column so that I don't have to be dummying those out. It seems reasonable that the starring actors should have some sort of effect on the success of a show, whether due to popularity or quality of acting. \n",
    "\n",
    "I will start by making a temporary dataframe to CountVectorize, then pass it back through the main Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_df = clean_series_df[['writer', 'overview_x', 'number_of_episodes', 'number_of_seasons', \n",
    "                     'overview_y', 'status_y', 'actors', 'awards', 'genre_y', 'imdb_rating',\n",
    "                     'imdb_votes', 'plot', 'runtime_x', 'runtime_cat', 'network']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "nlp_df[['runtime_x', 'awards']] = nlp_df[['runtime_x', 'awards']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make sure I don't have any null values and verify the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "nlp_df['overview_x'].fillna('N/A', inplace=True)\n",
    "nlp_df['overview_y'].fillna('N/A', inplace=True)\n",
    "nlp_df['plot'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "nlp_df['plot'] = nlp_df['plot'].astype(str)\n",
    "nlp_df['overview_x'] = nlp_df['overview_x'].astype(str)\n",
    "nlp_df['overview_y'] = nlp_df['overview_y'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "nlp_df['bag_of_words'] = nlp_df[['overview_x', 'plot','overview_y']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "nlp_df.drop(['overview_x', 'overview_y', 'plot'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['writer', 'number_of_episodes', 'number_of_seasons', 'status_y',\n",
       "       'actors', 'awards', 'genre_y', 'imdb_rating', 'imdb_votes', 'runtime_x',\n",
       "       'runtime_cat', 'network', 'bag_of_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = nlp_df[['actors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "test_df['imdb_votes'] = nlp_df[['imdb_votes']]\n",
    "test_df['imdb_rating'] = nlp_df[['imdb_rating']]\n",
    "test_df['number_of_episodes'] = nlp_df[['number_of_episodes']]\n",
    "test_df['number_of_seasons'] = nlp_df[['number_of_seasons']]\n",
    "test_df['awards'] = nlp_df[['awards']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['actors'] = test_df['actors'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that my data is cleaner, I can CountVectorize the actor names. I am using CountVectorizier over TF-IDF because this dataset/corpus is relatively small and I will want all of the actors to be seen; if I were to TF-IDF these names, I may just receive a shorter list of the strongest features.\n",
    "\n",
    "For the sake of avoiding actors with the same first or last names, I will be combining their first and last name along with removing punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=None, analyzer='word', \n",
    "                     ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "# List of strings\n",
    "name_list = []\n",
    "\n",
    "for row in test_df['actors']:\n",
    "    name_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list_clean = []\n",
    "for item in range(0, len(name_list)):\n",
    "    name_list_clean.append(name_list[item].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pm_slot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eb4317fc4f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mpm_slot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpm_slot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'airsTime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'afternoon'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpm_slot_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpm_slot_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'airsTime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'afternoon'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pm_slot' is not defined"
     ]
    }
   ],
   "source": [
    "# Categorizing PM slots\n",
    "for i in range(1, 13):\n",
    "    if i < 6:\n",
    "        pm_slot[pm_slot['airsTime'].str.contains(str(i))] = 'afternoon'\n",
    "        pm_slot_[pm_slot_['airsTime'].str.contains(str(i))] = 'afternoon'\n",
    "    elif i >= 6 & i < 10:\n",
    "        pm_slot[pm_slot['airsTime'].str.contains(str(i))] = 'evening'\n",
    "        pm_slot_[pm_slot_['airsTime'].str.contains(str(i))] = 'evening'\n",
    "    else: \n",
    "        pm_slot[pm_slot['airsTime'].str.contains(str(i))] = 'latenight'\n",
    "        pm_slot_[pm_slot_['airsTime'].str.contains(str(i))] = 'latenight'\n",
    "\n",
    "#pm_slot[pm_slot['airsTime'].str.contains('8', '9')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing PM slots\n",
    "for i in range(1, 13):\n",
    "    if i < 5:\n",
    "        am_slot[am_slot['airsTime'].str.contains(str(i))] = 'latenight'\n",
    "        am_slot_[am_slot_['airsTime'].str.contains(str(i))] = 'latenight'\n",
    "    else: \n",
    "        am_slot[am_slot['airsTime'].str.contains(str(i))] = 'morning'\n",
    "        am_slot_[am_slot_['airsTime'].str.contains(str(i))] = 'morning'\n",
    "\n",
    "#pm_slot[pm_slot['airsTime'].str.contains('8', '9')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_time['airsTime'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeslot = pd.concat((am_slot, am_slot_, pm_slot, pm_slot_, remaining_time, missing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day = series_df[['released']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day['released'] = pd.to_datetime(week_day['released'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day['weekday'] = week_day['released'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_day.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day['month'] = week_day['released'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "week_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizing the actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = cv.fit(name_list_clean)\n",
    "bag_of_words = cv.transform(name_list_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have a list of vectorized values for each actor, and will want to iterate through the test_df & multiply the imdb rating by this value (might want to scale it down?), then remove the actual actors themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actors_split = pd.concat([test_df['actors'].str.split(', ', expand=True)], axis=1)\n",
    "#actors_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actors_split = pd.concat([test_df['actors'].str.split(', ', expand=True)], axis=1)\n",
    "test_df = pd.concat((test_df ,actors_split), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[1] = test_df[1].fillna(\"none\")\n",
    "test_df[2] = test_df[2].fillna(\"none\")\n",
    "test_df[3] = test_df[3].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in range(0, len(test_df[0])):\n",
    "    test_df['actor_1'] = test_df[0][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(test_df[1])):\n",
    "    test_df['actor_2'] = test_df[1][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(test_df[2])):\n",
    "    test_df['actor_3'] = test_df[2][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(test_df[3])):\n",
    "    test_df['actor_4'] = test_df[3][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in range(0, len(test_df[0])):\n",
    "    test_df['actor_1'][number] = test_df[0][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(test_df[1])):\n",
    "    test_df['actor_2'][number] = test_df[1][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(test_df[2])):\n",
    "    test_df['actor_3'][number] = test_df[2][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(test_df[3])):\n",
    "    test_df['actor_4'][number] = test_df[3][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[2] = df[0].map(match).fillna(df[2])\n",
    "test_df['actor_1'] = test_df['actor_1'].map(match).fillna(test_df['actor_1'])\n",
    "test_df['actor_2'] = test_df['actor_2'].map(match).fillna(test_df['actor_2'])\n",
    "test_df['actor_3'] = test_df['actor_3'].map(match).fillna(test_df['actor_3'])\n",
    "test_df['actor_4'] = test_df['actor_4'].map(match).fillna(test_df['actor_4'])\n",
    "\n",
    "# This may allow me to replace the values in the cell with the count vectorized values, \n",
    "# but I'll need to have the words match first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df.drop(['actors', 0, 1, 2, 3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nones(df):\n",
    "    for i in range(0, len(df)):\n",
    "        if df[i] == 'none':\n",
    "            df[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_nones(temp_df['actor_2'])\n",
    "remove_nones(temp_df['actor_3'])\n",
    "remove_nones(temp_df['actor_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['actor_1'] = temp_df['actor_1'].astype(int)\n",
    "temp_df['actor_2'] = temp_df['actor_2'].astype(int)\n",
    "temp_df['actor_3'] = temp_df['actor_3'].astype(int)\n",
    "temp_df['actor_4'] = temp_df['actor_4'].astype(int)\n",
    "temp_df['imdb_rating'] = temp_df['imdb_rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['actor_1_weighted'] = temp_df['imdb_rating'] * temp_df['actor_1']\n",
    "temp_df['actor_2_weighted'] = temp_df['imdb_rating'] * temp_df['actor_2']\n",
    "temp_df['actor_3_weighted'] = temp_df['imdb_rating'] * temp_df['actor_3']\n",
    "temp_df['actor_4_weighted'] = temp_df['imdb_rating'] * temp_df['actor_4']\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['actors_cum_sum'] = temp_df['actor_1_weighted'] + temp_df['actor_2_weighted'] + temp_df['actor_3_weighted'] + temp_df['actor_4_weighted'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this space to assign weight to genre the same way that the weights were given to actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibly do the same for networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genre_df.drop(['writer', 'number_of_episodes', 'number_of_seasons', 'status_y',\n",
    "       'actors', 'awards', 'genre_y', 'imdb_rating', 'imdb_votes', 'runtime_x',\n",
    "       'runtime_cat', 'network', 'bag_of_words'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genre_df = nlp_df[['genre_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df['imdb_rating'] = nlp_df[['imdb_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df['genre_y'] = genre_df['genre_y'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=None, analyzer='word', \n",
    "                     ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "# List of strings\n",
    "genre_list = []\n",
    "\n",
    "for row in genre_df['genre_y']:\n",
    "    genre_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list_clean = []\n",
    "for item in range(0, len(genre_list)):\n",
    "    genre_list_clean.append(genre_list[item].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_count = cv.fit(genre_list_clean)\n",
    "genre_bag_words = cv.transform(genre_list_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_match = cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_split = pd.concat([genre_df['genre_y'].str.split(', ', expand=True)], axis=1)\n",
    "genre_df = pd.concat((genre_df, genre_split), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df[1] = genre_df[1].fillna(\"none\")\n",
    "genre_df[2] = genre_df[2].fillna(\"none\")\n",
    "genre_df[3] = genre_df[3].fillna(\"none\")\n",
    "genre_df[4] = genre_df[4].fillna(\"none\")\n",
    "genre_df[5] = genre_df[5].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_1'] = genre_df[0][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_2'] = genre_df[1][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_3'] = genre_df[2][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_4'] = genre_df[3][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_5'] = genre_df[4][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_6'] = genre_df[5][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_1'][number] = genre_df[0][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_2'][number] = genre_df[1][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_3'][number] = genre_df[2][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_4'][number] = genre_df[3][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_5'][number] = genre_df[4][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n",
    "for number in range(0, len(genre_df)):\n",
    "    genre_df['genre_6'][number] = genre_df[5][number].lower().replace(\"'\", \"\").replace(' ', '').replace('.', '').replace('-', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df['genre_1'] = genre_df['genre_1'].map(genre_match).fillna(genre_df['genre_1'])\n",
    "genre_df['genre_2'] = genre_df['genre_2'].map(genre_match).fillna(genre_df['genre_2'])\n",
    "genre_df['genre_3'] = genre_df['genre_3'].map(genre_match).fillna(genre_df['genre_3'])\n",
    "genre_df['genre_4'] = genre_df['genre_4'].map(genre_match).fillna(genre_df['genre_4'])\n",
    "genre_df['genre_5'] = genre_df['genre_5'].map(genre_match).fillna(genre_df['genre_5'])\n",
    "genre_df['genre_6'] = genre_df['genre_6'].map(genre_match).fillna(genre_df['genre_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = genre_df.drop(['genre_y', 0, 1, 2, 3, 4, 5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genre_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_nones(genre_df['genre_2'])\n",
    "remove_nones(genre_df['genre_3'])\n",
    "remove_nones(genre_df['genre_4'])\n",
    "remove_nones(genre_df['genre_5'])\n",
    "remove_nones(genre_df['genre_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df['genre_1'] = genre_df['genre_1'].astype(int)\n",
    "genre_df['genre_2'] = genre_df['genre_2'].astype(int)\n",
    "genre_df['genre_3'] = genre_df['genre_3'].astype(int)\n",
    "genre_df['genre_4'] = genre_df['genre_4'].astype(int)\n",
    "genre_df['genre_5'] = genre_df['genre_5'].astype(int)\n",
    "genre_df['genre_6'] = genre_df['genre_6'].astype(int)\n",
    "genre_df['imdb_rating'] = genre_df['imdb_rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df['genre_1_weighted'] = genre_df['imdb_rating'] * genre_df['genre_1']\n",
    "genre_df['genre_2_weighted'] = genre_df['imdb_rating'] * genre_df['genre_2']\n",
    "genre_df['genre_3_weighted'] = genre_df['imdb_rating'] * genre_df['genre_3']\n",
    "genre_df['genre_4_weighted'] = genre_df['imdb_rating'] * genre_df['genre_4']\n",
    "genre_df['genre_5_weighted'] = genre_df['imdb_rating'] * genre_df['genre_5']\n",
    "genre_df['genre_6_weighted'] = genre_df['imdb_rating'] * genre_df['genre_6']\n",
    "\n",
    "genre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df_weighted = genre_df.drop(['genre_1', 'genre_2', 'genre_3', 'genre_4', 'genre_5', 'genre_6'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Assets_&_Data/model_prelim.pickle', 'rb') as handle:\n",
    "    model_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = genre_df_weighted.drop('imdb_rating', axis=1)\n",
    "y = genre_df_weighted['imdb_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df[['Action', ' Adventure', ' Animation', ' Comedy', ' Crime',\n",
    "       ' Drama', ' Family', ' Fantasy', ' Game-Show', ' History', ' Horror',\n",
    "       ' Music', ' Musical', ' Mystery', ' News', ' Reality-TV', ' Romance',\n",
    "       ' Sci-Fi', ' Short', ' Sport', ' Talk-Show', ' Thriller', ' War',\n",
    "       ' Western', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy',\n",
    "       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show',\n",
    "       'History', 'Horror', 'Music', 'Mystery', 'News', 'Reality-TV',\n",
    "       'Romance', 'Sci-Fi', 'Sport', 'Talk-Show', 'Western']]\n",
    "y = model_df['imdb_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)\n",
    "lr.score(X_train_sc, y_train), lr.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = temp_df.drop(['imdb_rating', 'actor_1', 'actor_2', 'actor_3', 'actor_4'], axis=1)\n",
    "y = temp_df['imdb_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train_sc, y_train)\n",
    "\n",
    "lr.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp_df['raked_words'] = \"\"\n",
    "\n",
    "for index, row in nlp_df.iterrows():\n",
    "    plot = row['bag_of_words']\n",
    "    \n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(plot)\n",
    "\n",
    "    key_words_dict_scores = r.get_word_degrees()\n",
    "    row['raked_words'] = list(key_words_dict_scores)\n",
    "\n",
    "# dropping the Plot column\n",
    "#df.drop(columns = ['Plot'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake()\n",
    "r.extract_keywords_from_text(nlp_df['bag_of_words'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r.get_ranked_phrases_with_scores()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model = model_df.merge(temp_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_model = good_model.merge(network_dummies, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = good_model[['imdb_votes_x', 'awards_x', 'number_of_seasons_x', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4',\n",
    "       'day_5', 'day_6', ' Action', ' Adventure', ' Animation', ' Comedy', ' Crime',\n",
    "       ' Drama', ' Family', ' Fantasy', ' Game-Show', ' History', ' Horror',\n",
    "       ' Music', ' Musical', ' Mystery', ' News', ' Reality-TV', ' Romance',\n",
    "       ' Sci-Fi', ' Short', ' Sport', ' Talk-Show', ' Thriller', ' War',\n",
    "       ' Western', 'Action', 'Adventure', 'Animation', 'Biography', 'Comedy',\n",
    "       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Game-Show',\n",
    "       'History', 'Horror', 'Music', 'Mystery', 'News', 'Reality-TV',\n",
    "       'Romance', 'Sci-Fi', 'Sport', 'Talk-Show', 'Western', 'actor_1_weighted', 'actor_2_weighted', 'actor_3_weighted',\n",
    "       'actor_4_weighted']]\n",
    "y = good_model['imdb_rating_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=24, test_size=0.1)\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train_sc, y_train), lr.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train_sc, y_train)\n",
    "rf.score(X_train_sc, y_train), rf.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GradientBoostingRegressor()\n",
    "gs.fit(X_train_sc, y_train)\n",
    "gs.score(X_train_sc, y_train), gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next is genre bag-of-words for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writer bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Grid Search(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instantiate the models being used in the pipeline along with the static parameters\n",
    "2. Create parameter dicts for each of the models' hyperparameters\n",
    "3. Instantiate GridSearchCV with the above pipeline and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs = GridSearchCV(LinearRegression(), \n",
    "                         {'n_jobs': [1, 5, 10]}, \n",
    "                         n_jobs=10,\n",
    "                         scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs.score(X_train_sc, y_train), lr_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs = GridSearchCV(LassoCV(),\n",
    "                        {'n_alphas': [100, 250],\n",
    "                        'tol': [.0001, .001, .01]},\n",
    "                        scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs.fit(X_train_sc, y_train)\n",
    "lasso_gs.score(X_train_sc, y_train), lasso_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_gs = GridSearchCV(RidgeCV(),\n",
    "                        {'alphas': [(0.1, 1.0, 10.0), (0.01, 1.0, 10.0)]},\n",
    "                        scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_gs.fit(X_train_sc, y_train)\n",
    "ridge_gs.score(X_train_sc, y_train), ridge_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_gs = GridSearchCV(ElasticNetCV(),\n",
    "                        {'l1_ratio': [0.25, 0.5, 0.75],\n",
    "                         'n_alphas': [100, 250],\n",
    "                         'tol': [0.0001, 0.001, 0.01]},\n",
    "                     scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_gs.fit(X_train_sc, y_train)\n",
    "en_gs.score(X_train_sc, y_train), en_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs = GridSearchCV(RandomForestRegressor(), \n",
    "                         {'n_estimators': [10, 100, 200],\n",
    "                         'max_depth': [5, 25, 50],\n",
    "                         'min_samples_split': [2, 5, 10],\n",
    "                         'n_jobs': [1, 5, 10]},\n",
    "                     scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs.fit(X_train_sc, y_train)\n",
    "rf_gs.score(X_train_sc, y_train), rf_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_gs = GridSearchCV(GradientBoostingRegressor(),\n",
    "                        {'learning_rate': [0.01, 0.1, 0.15],\n",
    "                        'n_estimators': [100, 250],\n",
    "                        'min_samples_split': [2, 4, 6],\n",
    "                        'max_depth': [3, 5, 10],\n",
    "                        'max_features': [0.5, 0.75, None],\n",
    "                        'alpha': [0.9, 0.95]},\n",
    "                     scoring='neg_mean_absolute_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_gs.fit(X_train_sc, y_train)\n",
    "gb_gs.score(X_train_sc, y_train), gb_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train_sc, y_train)\n",
    "gs.score(X_train_sc, y_train), gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gb_gs.fit(X_train_sc, y_train)\n",
    "gb_gs.score(X_train_sc, y_train), gb_gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(good_model['imdb_rating_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(good_model['imdb_votes_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_model.imdb_votes_x.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(good_model['imdb_rating_x'], good_model['imdb_votes_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(series_df['imdb_rating'], series_df['number_of_seasons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(series_df['imdb_rating'], series_df['number_of_episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_shows = list(model_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Assets_&_Data/final_show_list.pickle', 'wb') as f:\n",
    "    pickle.dump(final_shows, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
