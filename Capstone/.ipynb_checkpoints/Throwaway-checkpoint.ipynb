{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Band_of_Brothers_episodes\"\n",
    "res = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a for loop to create URLs using the format \"List_of_SHOW_NAME_episodes\"\n",
    "# also format for seasons\n",
    "# This will grab viewership info, loglines, writers, number of episodes, number of seasons, air dates?\n",
    "# https://en.wikipedia.org/wiki/SHOW_NAME_(season_#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function re.findall(pattern, string, flags=0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', {'class':'wikitable plainrowheaders wikiepisodetable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewership = []\n",
    "table = soup.find('table', {'class':'wikitable plainrowheaders wikiepisodetable'})\n",
    "row = soup.find('tr')\n",
    "for row in table.find_all('tr', {'class': 'vevent'}):\n",
    "    rating = {}\n",
    "    rating['name'] = row.text\n",
    "#    rating['slug'] = row['tr'].split('/')[-1]\n",
    "    viewership.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in /anaconda3/lib/python3.6/site-packages (from wikipedia) (4.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /anaconda3/lib/python3.6/site-packages (from wikipedia) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2018.4.16)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Running setup.py bdist_wheel for wikipedia ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/eyoo/Library/Caches/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wikipedia as w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@Alexander_H/scraping-wikipedia-with-python-8000fc9c9e6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.thetvdb.com\n",
    "# username : ericyyoo0016tf\n",
    "# unique id : J3E04YJ3CJ2KT7UO\n",
    "# API Key : I4NH1SXHARPKRI2P\n",
    "# This API will grab writers, summaries, air date, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.trakt.tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
